"""
Text Encoder í•™ìŠµ ê²€ì¦ì„ ìœ„í•œ í‰ê°€ ëª¨ë“ˆ
"""
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
from typing import Dict, List, Tuple, Optional
import json
import os
from tqdm import tqdm

from models.text_encoder import TextEncoder, TextDecoder, TextEncoderTrainer
from config import LanHARConfig

class TextEncoderEvaluator:
    """Text Encoder í•™ìŠµ ê²€ì¦ í´ë˜ìŠ¤"""
    
    def __init__(self, config: LanHARConfig, text_encoder: Optional[TextEncoder] = None, model_path: Optional[str] = None):
        self.config = config
        self.device = torch.device(config.device)
        
        # ëª¨ë¸ ë¡œë“œ
        if text_encoder is not None:
            self.text_encoder = text_encoder
            self.text_decoder = TextDecoder(config).to(self.device)
            print(f"âœ… TextEncoder ê°ì²´ë¥¼ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        elif model_path and os.path.exists(model_path):
            self.text_encoder = TextEncoder(config).to(self.device)
            self.text_decoder = TextDecoder(config).to(self.device)
            self.load_model(model_path)
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
        else:
            self.text_encoder = TextEncoder(config).to(self.device)
            self.text_decoder = TextDecoder(config).to(self.device)
            print("âš ï¸  ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ëœë¤ ì´ˆê¸°í™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    def load_model(self, model_path: str):
        """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(model_path, map_location=self.device)
        if 'text_encoder' in checkpoint:
            self.text_encoder.load_state_dict(checkpoint['text_encoder'])
        else:
            self.text_encoder.load_state_dict(checkpoint)
    
    def evaluate_alignment_quality(self, sensor_interpretations: List[str], 
                                 activity_interpretations: List[str]) -> Dict[str, float]:
        """Sensor-Activity ì •ë ¬ í’ˆì§ˆ í‰ê°€"""
        print("ğŸ” Sensor-Activity ì •ë ¬ í’ˆì§ˆ í‰ê°€ ì¤‘...")
        
        # ì„ë² ë”© ìƒì„±
        sensor_embeddings = self.text_encoder(sensor_interpretations)
        activity_embeddings = self.text_encoder(activity_interpretations)
        
        # ì •ê·œí™”
        sensor_embeddings = F.normalize(sensor_embeddings, p=2, dim=1)
        activity_embeddings = F.normalize(activity_embeddings, p=2, dim=1)
        
        # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        similarity_matrix = torch.matmul(sensor_embeddings, activity_embeddings.T)
        
        # ëŒ€ê°ì„  ìš”ì†Œ (ì •ë‹µ ìŒ)ì˜ ìœ ì‚¬ë„
        diagonal_similarities = torch.diag(similarity_matrix)
        
        # ê° í–‰ì—ì„œ ìµœê³  ìœ ì‚¬ë„ (ì •ë‹µì´ ìµœê³ ì¸ì§€ í™•ì¸)
        max_similarities, max_indices = torch.max(similarity_matrix, dim=1)
        
        # ì •ë‹µë¥  ê³„ì‚°
        correct_predictions = (max_indices == torch.arange(len(sensor_interpretations), device=self.device)).float()
        accuracy = correct_predictions.mean().item()
        
        # í‰ê·  ì •ë‹µ ìŒ ìœ ì‚¬ë„
        avg_correct_similarity = diagonal_similarities.mean().item()
        
        # ì •ë‹µê³¼ ë¹„ì •ë‹µ ê°„ ìœ ì‚¬ë„ ì°¨ì´
        off_diagonal_similarities = similarity_matrix - torch.diag(diagonal_similarities)
        avg_incorrect_similarity = off_diagonal_similarities.mean().item()
        
        margin = avg_correct_similarity - avg_incorrect_similarity
        
        return {
            'accuracy': accuracy,
            'avg_correct_similarity': avg_correct_similarity,
            'avg_incorrect_similarity': avg_incorrect_similarity,
            'margin': margin,
            'diagonal_similarities': diagonal_similarities.detach().cpu().numpy(),
            'similarity_matrix': similarity_matrix.detach().cpu().numpy()
        }
    
    def evaluate_reconstruction_quality(self, texts: List[str]) -> Dict[str, float]:
        """ì¬êµ¬ì„± í’ˆì§ˆ í‰ê°€"""
        print("ğŸ” ì¬êµ¬ì„± í’ˆì§ˆ í‰ê°€ ì¤‘...")
        
        # ì„ë² ë”© ìƒì„±
        embeddings = self.text_encoder(texts)
        
        reconstruction_losses = []
        reconstruction_accuracies = []
        
        for i, text in enumerate(texts):
            try:
                # í† í°í™”
                tokens = self.text_encoder.tokenizer.encode(
                    text, 
                    add_special_tokens=True, 
                    max_length=self.config.max_sequence_length,
                    truncation=True
                )
                
                # ì¬êµ¬ì„± ì‹œë„
                input_tokens = torch.tensor([tokens[:-1]], device=self.device)  # ë§ˆì§€ë§‰ í† í° ì œì™¸
                target_tokens = torch.tensor([tokens[1:]], device=self.device)   # ì²« í† í° ì œì™¸
                
                # ì¬êµ¬ì„±
                decoder_output = self.text_decoder(embeddings[i:i+1], input_tokens)
                
                # ì†ì‹¤ ê³„ì‚°
                loss = F.cross_entropy(
                    decoder_output.reshape(-1, decoder_output.size(-1)),
                    target_tokens.reshape(-1),
                    ignore_index=self.text_encoder.tokenizer.pad_token_id
                )
                
                reconstruction_losses.append(loss.item())
                
                # ì •í™•ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
                predicted_tokens = torch.argmax(decoder_output, dim=-1)
                accuracy = (predicted_tokens == target_tokens).float().mean().item()
                reconstruction_accuracies.append(accuracy)
                
            except Exception as e:
                print(f"ì¬êµ¬ì„± ì˜¤ë¥˜ (í…ìŠ¤íŠ¸ {i}): {e}")
                reconstruction_losses.append(float('inf'))
                reconstruction_accuracies.append(0.0)
        
        return {
            'avg_reconstruction_loss': np.mean(reconstruction_losses),
            'avg_reconstruction_accuracy': np.mean(reconstruction_accuracies),
            'reconstruction_losses': reconstruction_losses,
            'reconstruction_accuracies': reconstruction_accuracies
        }
    
    def visualize_embeddings(self, sensor_interpretations: List[str], 
                           activity_interpretations: List[str],
                           activities: List[str],
                           save_path: str = "outputs/embedding_visualization.png"):
        """ì„ë² ë”© ì‹œê°í™”"""
        print("ğŸ“Š ì„ë² ë”© ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # ì„ë² ë”© ìƒì„±
        sensor_embeddings = self.text_encoder(sensor_interpretations)
        activity_embeddings = self.text_encoder(activity_interpretations)
        
        # t-SNEë¡œ ì°¨ì› ì¶•ì†Œ
        all_embeddings = torch.cat([sensor_embeddings, activity_embeddings], dim=0)
        all_embeddings_np = all_embeddings.detach().cpu().numpy()
        
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_embeddings_np)-1))
        embeddings_2d = tsne.fit_transform(all_embeddings_np)
        
        # ì‹œê°í™”
        plt.figure(figsize=(15, 10))
        
        # Sensor embeddings
        sensor_2d = embeddings_2d[:len(sensor_interpretations)]
        scatter1 = plt.scatter(sensor_2d[:, 0], sensor_2d[:, 1], 
                              c='blue', alpha=0.6, s=50, label='Sensor Interpretations')
        
        # Activity embeddings
        activity_2d = embeddings_2d[len(sensor_interpretations):]
        scatter2 = plt.scatter(activity_2d[:, 0], activity_2d[:, 1], 
                              c='red', alpha=0.6, s=50, label='Activity Interpretations')
        
        # ì—°ê²°ì„  ê·¸ë¦¬ê¸° (ì •ë‹µ ìŒ)
        for i in range(len(sensor_interpretations)):
            plt.plot([sensor_2d[i, 0], activity_2d[i, 0]], 
                    [sensor_2d[i, 1], activity_2d[i, 1]], 
                    'k--', alpha=0.3, linewidth=0.5)
        
        plt.title('Text Encoder Embeddings Visualization')
        plt.xlabel('t-SNE Dimension 1')
        plt.ylabel('t-SNE Dimension 2')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ì €ì¥
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")
    
    def evaluate_similarity_matrix(self, sensor_interpretations: List[str], 
                                 activity_interpretations: List[str],
                                 save_path: str = "outputs/similarity_matrix.png"):
        """ìœ ì‚¬ë„ í–‰ë ¬ ì‹œê°í™”"""
        print("ğŸ“Š ìœ ì‚¬ë„ í–‰ë ¬ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        # ì„ë² ë”© ìƒì„±
        sensor_embeddings = self.text_encoder(sensor_interpretations)
        activity_embeddings = self.text_encoder(activity_interpretations)
        
        # ì •ê·œí™”
        sensor_embeddings = F.normalize(sensor_embeddings, p=2, dim=1)
        activity_embeddings = F.normalize(activity_embeddings, p=2, dim=1)
        
        # ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        similarity_matrix = torch.matmul(sensor_embeddings, activity_embeddings.T)
        similarity_matrix_np = similarity_matrix.detach().cpu().numpy()
        
        # ì‹œê°í™”
        plt.figure(figsize=(10, 8))
        sns.heatmap(similarity_matrix_np, 
                   annot=True, 
                   fmt='.2f', 
                   cmap='RdYlBu_r',
                   center=0,
                   square=True)
        
        plt.title('Sensor-Activity Similarity Matrix')
        plt.xlabel('Activity Interpretations')
        plt.ylabel('Sensor Interpretations')
        
        # ì €ì¥
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… ìœ ì‚¬ë„ í–‰ë ¬ ì €ì¥ ì™„ë£Œ: {save_path}")
    
    def comprehensive_evaluation(self, interpretations_file: str, 
                              output_dir: str = "outputs") -> Dict:
        """ì¢…í•© í‰ê°€"""
        print("ğŸš€ Text Encoder ì¢…í•© í‰ê°€ ì‹œì‘...")
        
        # ë°ì´í„° ë¡œë“œ
        with open(interpretations_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Sensor interpretations ì¶”ì¶œ
        sensor_interpretations = []
        activity_interpretations = []
        activities = []
        
        for home_id, home_data in data['sensor_interpretations'].items():
            for split, windows in home_data.items():
                if split == 'train':  # train ë°ì´í„°ë§Œ ì‚¬ìš©
                    for window_id, window_data in windows.items():
                        if 'interpretation' in window_data:
                            sensor_interpretations.append(window_data['interpretation'])
                            activities.append(window_data['activity'])
        
        # Activity interpretations ì¶”ì¶œ
        for activity, interpretation_data in data.get('activity_interpretations', {}).items():
            if 'interpretation' in interpretation_data:
                activity_interpretations.append(interpretation_data['interpretation'])
        
        # ë°ì´í„° ìˆ˜ ì œí•œ (í‰ê°€ìš©)
        max_samples = min(50, len(sensor_interpretations))
        sensor_interpretations = sensor_interpretations[:max_samples]
        activities = activities[:max_samples]
        
        # Activity interpretationsë„ ë§¤ì¹­
        activity_interpretations = activity_interpretations[:max_samples]
        
        print(f"ğŸ“Š í‰ê°€ ë°ì´í„°: {len(sensor_interpretations)}ê°œ sensor, {len(activity_interpretations)}ê°œ activity")
        
        # 1. ì •ë ¬ í’ˆì§ˆ í‰ê°€
        alignment_results = self.evaluate_alignment_quality(
            sensor_interpretations, activity_interpretations
        )
        
        # 2. ì¬êµ¬ì„± í’ˆì§ˆ í‰ê°€
        reconstruction_results = self.evaluate_reconstruction_quality(
            sensor_interpretations[:10]  # ì¬êµ¬ì„±ì€ ì¼ë¶€ë§Œ í…ŒìŠ¤íŠ¸
        )
        
        # 3. ì‹œê°í™”
        self.visualize_embeddings(
            sensor_interpretations, activity_interpretations, activities,
            os.path.join(output_dir, "embedding_visualization.png")
        )
        
        self.evaluate_similarity_matrix(
            sensor_interpretations, activity_interpretations,
            os.path.join(output_dir, "similarity_matrix.png")
        )
        
        # ê²°ê³¼ ì¢…í•©
        results = {
            'alignment_quality': alignment_results,
            'reconstruction_quality': reconstruction_results,
            'evaluation_summary': {
                'accuracy': alignment_results['accuracy'],
                'margin': alignment_results['margin'],
                'reconstruction_loss': reconstruction_results['avg_reconstruction_loss'],
                'reconstruction_accuracy': reconstruction_results['avg_reconstruction_accuracy']
            }
        }
        
        # ê²°ê³¼ ì €ì¥ (numpy arraysë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜)
        def convert_numpy_types(obj):
            if isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            else:
                return obj
        
        results_serializable = convert_numpy_types(results)
        
        results_file = os.path.join(output_dir, "text_encoder_evaluation_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_serializable, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… í‰ê°€ ê²°ê³¼ ì €ì¥: {results_file}")
        
        return results

def test_text_encoder_evaluation():
    """Text Encoder í‰ê°€ í…ŒìŠ¤íŠ¸"""
    from config import LanHARConfig
    
    config = LanHARConfig()
    
    # ëª¨ë¸ ê²½ë¡œ í™•ì¸
    model_path = "checkpoints/text_encoder_trained.pth"
    if not os.path.exists(model_path):
        print("âš ï¸  í•™ìŠµëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ëœë¤ ì´ˆê¸°í™”ëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
        model_path = None
    
    # í‰ê°€ê¸° ì´ˆê¸°í™”
    evaluator = TextEncoderEvaluator(config, model_path)
    
    # Interpretations íŒŒì¼ ê²½ë¡œ
    interpretations_file = "outputs/batch_semantic_interpretations_20250922_091130.json"
    
    if os.path.exists(interpretations_file):
        # ì¢…í•© í‰ê°€ ì‹¤í–‰
        results = evaluator.comprehensive_evaluation(interpretations_file)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ“Š TEXT ENCODER í‰ê°€ ê²°ê³¼")
        print("="*50)
        print(f"ì •í™•ë„: {results['evaluation_summary']['accuracy']:.3f}")
        print(f"ë§ˆì§„: {results['evaluation_summary']['margin']:.3f}")
        print(f"ì¬êµ¬ì„± ì†ì‹¤: {results['evaluation_summary']['reconstruction_loss']:.3f}")
        print(f"ì¬êµ¬ì„± ì •í™•ë„: {results['evaluation_summary']['reconstruction_accuracy']:.3f}")
        print("="*50)
        
    else:
        print(f"âŒ Interpretations íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {interpretations_file}")

if __name__ == "__main__":
    test_text_encoder_evaluation()
